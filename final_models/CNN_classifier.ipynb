{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla CNN Model\n",
    "    apply for both binary and 3-class classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "dt=pd.read_csv(\"samsung_final_14may.csv\", encoding='utf-8',\n",
    "               usecols=['id','text_original','text_preprocessed','polarityNum'],skip_blank_lines=True)\n",
    "\n",
    "dt.drop(dt[dt.polarityNum == 0].index, inplace=True) # uncomment it for binary classification, to filter out neutral tweets\n",
    "X=np.array(dt['text_preprocessed'])\n",
    "y=np.array(dt['polarityNum']).astype('int')\n",
    "np.place(y,y==-1,(0)) # uncomment it for binary classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data into train,test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.model_selection import train_test_split\n",
    "x_train,x_test=train_test_split(X, test_size=0.1, random_state=4)\n",
    "y_train,y_test=train_test_split(y,test_size=0.1,random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19417 number of train samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(x_train), 'number of train samples')\n",
    "\n",
    "max_len=len(max(x_train, key=len))\n",
    "num_classes=len(np.unique(y_train))\n",
    "\n",
    "max_len = 100      # max length of input sequence\n",
    "#max_word=10000    # max words used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (19417, 100)\n",
      "x_test shape: (2158, 100)\n"
     ]
    }
   ],
   "source": [
    "## build the word index, uni-gram model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer=Tokenizer(split=\" \")\n",
    "#tokenizer=Tokenizer(num_words=max_word,split=\" \")\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "max_word=len(tokenizer.word_index)+1\n",
    "\n",
    "x_train=tokenizer.texts_to_sequences(x_train) # convert to np matrix\n",
    "x_test=tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "## convert x-train to np array by pad seq to fix the dim\n",
    "from keras.preprocessing import sequence\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train,max_len)\n",
    "x_test = sequence.pad_sequences(x_test,max_len)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment this cell for binary classification\n",
    "if num_classes==3:\n",
    "    print('Convert class vector to binary class matrix '\n",
    "          '(for use with categorical_crossentropy)')\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test_bi = keras.utils.to_categorical(y_test, num_classes)\n",
    "    print('y_train shape:', y_train.shape)\n",
    "    print('y_test shape:', y_test_bi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential  \n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout,Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.merge import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanilla cnn for multi-class classification\n",
    "def define_model_multi(length, vocab_size):\n",
    "    # set parameters:\n",
    "    embedding_dims = 300 #[100,150,300]\n",
    "    nb_filters = 128  #[64,128,250]\n",
    "    kernel_size = 3 #[3,7]\n",
    "    hidden_dims = 128 #[64,128,256]\n",
    "\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    inputs = Input(shape=(length,))\n",
    "    cnn=Embedding(max_word,\n",
    "                    embedding_dims,\n",
    "                    input_length=max_len)(inputs)\n",
    "          \n",
    "    cnn=Dropout(0.2)(cnn) #[0.2,0.5,0.8]\n",
    "\n",
    "    # we add a Convolution1D, which will learn filters\n",
    "    # word group filters of size filter_length:\n",
    "    cnn=Conv1D(nb_filters,\n",
    "                 kernel_size,\n",
    "                 activation='linear', #[relu,tanh,linear]\n",
    "                 strides=1)(cnn)\n",
    "    # we use max pooling:\n",
    "    cnn=GlobalMaxPooling1D()(cnn)\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    cnn=Dense(hidden_dims)(cnn)\n",
    "    cnn=Dropout(0.2)(cnn)\n",
    "    cnn=Activation('relu')(cnn)\n",
    "\n",
    "    outputs=Dense(3,activation='softmax')(cnn)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    # compile\n",
    "    model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanilla cnn for binary classification\n",
    "\n",
    "def define_model_binary(length, vocab_size):\n",
    "    # set parameters:\n",
    "    embedding_dims = 300 #[100,150,300]\n",
    "    nb_filters = 250  #[64,128,250]\n",
    "    kernel_size = 3 #[3,7]\n",
    "    hidden_dims = 250 #[64,128,250]\n",
    "\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    inputs = Input(shape=(length,))\n",
    "    cnn=Embedding(max_word,\n",
    "                    embedding_dims,\n",
    "                    input_length=max_len)(inputs)\n",
    "          \n",
    "    cnn=Dropout(0.2)(cnn) #[0.2,0.5,0.8]\n",
    "\n",
    "    # we add a Convolution1D, which will learn filters\n",
    "    # word group filters of size filter_length:\n",
    "    cnn=Conv1D(nb_filters,\n",
    "                 kernel_size,\n",
    "                 activation='relu', #[relu,tanh,linear]\n",
    "                 strides=1)(cnn)\n",
    "    # we use max pooling:\n",
    "    cnn=GlobalMaxPooling1D()(cnn)\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    cnn=Dense(hidden_dims)(cnn)\n",
    "    cnn=Dropout(0.2)(cnn)\n",
    "    cnn=Activation('relu')(cnn)\n",
    "\n",
    "    outputs=Dense(1,activation='sigmoid')(cnn)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    # compile\n",
    "    model.compile('adam', 'binary_crossentropy', metrics=['binary_accuracy']) #[rmsprop,adam]\n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit model and save intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 100, 300)          6486300   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 98, 250)           225250    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 6,774,551\n",
      "Trainable params: 6,774,551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train...\n",
      "Train on 17475 samples, validate on 1942 samples\n",
      "Epoch 1/1\n",
      "17475/17475 [==============================] - 119s 7ms/step - loss: 0.4342 - binary_accuracy: 0.7895 - val_loss: 0.3780 - val_binary_accuracy: 0.8290\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1 #optimal after earlystopping tuning\n",
    "batch_size = 64 #[32,64,128]\n",
    "# fit model\n",
    "if num_classes==3:\n",
    "    model=define_model_multi(max_len, max_word)\n",
    "elif num_classes==2:\n",
    "    model=define_model_binary(max_len, max_word)\n",
    "\n",
    "print('Train...')\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "es=EarlyStopping(monitor='val_loss', min_delta=0, patience=1, verbose=0, mode='auto')\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "mc = ModelCheckpoint('model\\\\weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', verbose=0, save_best_only=True, mode='min', period=1)\n",
    "history=model.fit(x_train, y_train, epochs=num_epochs, batch_size=batch_size,validation_split=0.1,callbacks=[es,mc]) #\n",
    "model.save('model\\\\wm_cnn.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2158/2158 [==============================] - 3s 1ms/step\n",
      "Test score: 0.38027802670410765\n",
      "Test accuracy: 0.8271547734903119\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('model\\\\wm_cnn.h5')\n",
    "# load weights with least var_loss into new model for comparision\n",
    "#model.load_weights(\"model\\\\weights0.01-0.64.hdf5\")\n",
    "\n",
    "#evaluate\n",
    "if num_classes==3:\n",
    "    score = model.evaluate(x_test, y_test_bi,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "elif num_classes==2:\n",
    "    score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "    \n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "y_pred=model.predict(x_test, batch_size=batch_size)\n",
    "\n",
    "if num_classes ==3:\n",
    "    y_pred=np.argmax(y_pred, axis=1)\n",
    "    np.place(y_pred,y_pred==2,(-1))\n",
    "elif num_classes==2:\n",
    "    y_pred=y_pred.flatten().tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for 2 classes: 0.8329641116526364\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.77      0.82      1096\n",
      "          1       0.79      0.89      0.83      1062\n",
      "\n",
      "avg / total       0.83      0.83      0.82      2158\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score,classification_report\n",
    "if num_classes==2:\n",
    "    from decimal import Decimal, ROUND_HALF_EVEN\n",
    "    threshold = Decimal(0.5).quantize(Decimal('.01'), rounding=ROUND_HALF_EVEN)\n",
    "    y_output=[]\n",
    "    for i in range(len(y_pred)):\n",
    "        a = Decimal(y_pred[i]).quantize(Decimal('.01'), rounding=ROUND_HALF_EVEN)\n",
    "        Diff = a - threshold\n",
    "        cmp = Decimal(0.00).quantize(Decimal('.01'), rounding=ROUND_HALF_EVEN)\n",
    "\n",
    "        if Diff.compare(cmp) >= 0:\n",
    "            y_output.append(1)\n",
    "        else:\n",
    "            y_output.append(0)\n",
    "    print('F1 score for 2 classes:', f1_score(y_test,y_output,average='binary'))\n",
    "    print(classification_report(y_test, y_output))\n",
    "\n",
    "elif num_classes==3:\n",
    "    print('F1-macro score for 3 classes:', f1_score(y_test,y_pred,average='macro'))\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
