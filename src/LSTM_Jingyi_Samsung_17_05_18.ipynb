{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32466,)\n",
      "(32466,)\n"
     ]
    }
   ],
   "source": [
    "''' use the one hold-out method for evaluation'''\n",
    "# load data\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dt=pd.read_csv(\"data\\\\samsung_final_14may.csv\", encoding='utf-8',\n",
    "               usecols=['id','text_original','text_preprocessed','polarityNum'],skip_blank_lines=True) \n",
    "X=np.array(dt['text_preprocessed'])\n",
    "y=np.array(dt['polarityNum']).astype('int')\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "from  sklearn.model_selection import train_test_split\n",
    "x_train,x_test=train_test_split(X, test_size=0.1, random_state=4)\n",
    "y_train,y_test=train_test_split(y,test_size=0.1,random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32466,)\n",
      "(32466,)\n"
     ]
    }
   ],
   "source": [
    "''' use the gold standard for evaluation'''\n",
    "# load data\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from  sklearn.model_selection import train_test_split\n",
    "\n",
    "dt=pd.read_csv(\"data\\\\samsung_final_14may.csv\", encoding='utf-8',\n",
    "               usecols=['id','text_original','text_preprocessed','polarityNum'],skip_blank_lines=True) \n",
    "x_train=np.array(dt['text_preprocessed'])\n",
    "y_train=np.array(dt['polarityNum']).astype('int')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "dt1=pd.read_csv(\"data\\\\samsung_test_final_14may.csv\", encoding='utf-8',\n",
    "               usecols=['id','text_original','text_preprocessed','polarityNum'],skip_blank_lines=True) \n",
    "x_test=np.array(dt1['text_preprocessed'])\n",
    "y_test=np.array(dt1['polarityNum']).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "29219 number of train samples\n"
     ]
    }
   ],
   "source": [
    "# some overview of the data, and change label to numpy arrary\n",
    "import numpy as np\n",
    "print('Loading data...')\n",
    "print(len(x_train), 'number of train samples')\n",
    "\n",
    "max_len=len(max(x_train, key=len))\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100   # [50,100,na]\n",
    "#max_word=10000    # original vocab_size=11315"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (29219, 100)\n",
      "x_test shape: (3247, 100)\n"
     ]
    }
   ],
   "source": [
    "## build the char index, uni-gram model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer=Tokenizer(split=\" \")\n",
    "#tokenizer=Tokenizer(num_words=max_word,split=\" \")\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "max_word=len(tokenizer.word_index)+1\n",
    "\n",
    "x_train=tokenizer.texts_to_sequences(x_train) # convert to np matrix\n",
    "x_test=tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "## convert x-train to np array by pad seq to fix the dim\n",
    "from keras.preprocessing import sequence\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train,max_len)\n",
    "x_test = sequence.pad_sequences(x_test,max_len)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
      "y_train shape: (29219, 3)\n",
      "y_test shape: (3247, 3)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print('Convert class vector to binary class matrix '\n",
    "      '(for use with categorical_crossentropy)')\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_bi = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test_bi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 # [32,64,128]\n",
    "num_epochs =5  # early stopping, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential  \n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout,Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.layers.merge import Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "    inputs = Input(shape=(length,))\n",
    "    lstm= Embedding(vocab_size, 300, input_length=length)(inputs)  #100,150,300\n",
    "    lstm=Bidirectional(LSTM(150))(lstm) #100,150,300\n",
    "    lstm=Dropout(0.5)(lstm) # [0.2,0.5,0.8]\n",
    "    outputs = Dense(num_classes, activation='softmax')(lstm)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    # compile\n",
    "    model.compile('adam', 'categorical_crossentropy', metrics=['accuracy']) #[adam,]\n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "    #plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 100, 300)          8664600   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 300)               541200    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 903       \n",
      "=================================================================\n",
      "Total params: 9,206,703\n",
      "Trainable params: 9,206,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train...\n",
      "Train on 26297 samples, validate on 2922 samples\n",
      "Epoch 1/5\n",
      "26297/26297 [==============================] - 818s 31ms/step - loss: 0.7558 - acc: 0.6656 - val_loss: 0.6340 - val_acc: 0.7361\n",
      "Epoch 2/5\n",
      "26297/26297 [==============================] - 788s 30ms/step - loss: 0.4501 - acc: 0.8236 - val_loss: 0.6239 - val_acc: 0.7474\n",
      "Epoch 3/5\n",
      "26297/26297 [==============================] - 701s 27ms/step - loss: 0.2977 - acc: 0.8871 - val_loss: 0.6824 - val_acc: 0.7522\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model=define_model(max_len, max_word)\n",
    "# fit model\n",
    "print('Train...')\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "es=EarlyStopping(monitor='val_loss', min_delta=0, patience=1, verbose=0, mode='auto')\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "mc = ModelCheckpoint('model\\\\weights2.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', verbose=0, save_best_only=True, mode='min', period=1)\n",
    "history=model.fit(x_train, y_train, epochs=num_epochs, batch_size=batch_size,validation_split=0.1,callbacks=[es,mc]) #\n",
    "model.save('model\\\\wm_lstm2.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3247/3247 [==============================] - 21s 7ms/step\n",
      "Test score: 0.5914144688912601\n",
      "Test accuracy: 0.765321835191719\n",
      "F1-macro score for 3 classes: 0.7644080622380697\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.74      0.85      0.79      1092\n",
      "          0       0.82      0.68      0.74      1162\n",
      "          1       0.74      0.78      0.76       993\n",
      "\n",
      "avg / total       0.77      0.77      0.76      3247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load weights with least var_loss into new model\n",
    "from keras.models import load_model\n",
    "model = load_model('model\\\\wm_lstm_d5_allwd_adam_e2.h5')\n",
    "#model.load_weights(\"model\\\\weights2.02-0.62.hdf5\")\n",
    "\n",
    "#evaluate\n",
    "score = model.evaluate(x_test, y_test_bi,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "y_pred=model.predict(x_test, batch_size=batch_size)\n",
    "y_pred=np.argmax(y_pred, axis=1)\n",
    "np.place(y_pred,y_pred==2,(-1))\n",
    "\n",
    "from sklearn.metrics import f1_score,classification_report\n",
    "print('F1-macro score for 3 classes:', f1_score(y_test,y_pred,average='macro'))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential             # save model\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json\n",
    "import numpy\n",
    "import os\n",
    "model.save('model\\\\wm_lstm_d5_allwd_adam_e2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
