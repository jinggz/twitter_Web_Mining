{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33290,)\n",
      "(33290,)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dt=pd.read_csv(\"data\\\\Merged_final.csv\", encoding='utf-8',\n",
    "               usecols=['text_preprocessed','polarityNum'],skip_blank_lines=True) # 33295 lines\n",
    "X=np.array(dt['text_preprocessed'])\n",
    "y=np.array(dt['polarityNum']).astype('int')\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from  sklearn.model_selection import train_test_split\n",
    "x_train,x_test=train_test_split(X, test_size=0.1, random_state=42)\n",
    "y_train,y_test=train_test_split(y,test_size=0.1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "29961 number of train samples\n"
     ]
    }
   ],
   "source": [
    "# some overview of the data, and change label to numpy arrary\n",
    "import numpy as np\n",
    "print('Loading data...')\n",
    "print(len(x_train), 'number of train samples')\n",
    "\n",
    "#max_len=len(max(x_train, key=len))\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "max_word=10000    # original vocab_size=11315"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\keras\\preprocessing\\text.py:172: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (29961, 100)\n",
      "x_test shape: (3329, 100)\n"
     ]
    }
   ],
   "source": [
    "## build the char index, uni-gram model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer=Tokenizer(nb_words=max_word,split=\" \")\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train=tokenizer.texts_to_sequences(x_train) # convert to np matrix\n",
    "x_test=tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "## convert x-train to np array by pad seq to fix the dim\n",
    "from keras.preprocessing import sequence\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train,max_len)\n",
    "x_test = sequence.pad_sequences(x_test,max_len)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
      "y_train shape: (29961, 3)\n",
      "y_test shape: (3329, 3)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print('Convert class vector to binary class matrix '\n",
    "      '(for use with categorical_crossentropy)')\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_bi = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test_bi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multi-channel CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers import MaxPooling1D,GlobalMaxPooling1D\n",
    "from keras.layers.merge import Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of cnn model\n",
    "\n",
    "filter_length=[5,3]\n",
    "nb_filters=[128,128]\n",
    "dropout_prob = [0.5,0.5]\n",
    "pool_length = [2,2]\n",
    "batch_size = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "    \n",
    "    # multi-channel cnn\n",
    "    channel=[]\n",
    "    inputs=[]\n",
    "    for i in range(len(filter_length)):\n",
    "        input_channel = Input(shape=(length,))\n",
    "        embedding = Embedding(vocab_size, 100)(input_channel)\n",
    "        conv = Conv1D(filters=nb_filters[i], kernel_size=filter_length[i], activation='relu')(embedding)\n",
    "        drop = Dropout(dropout_prob[i])(conv)\n",
    "        pool = MaxPooling1D(pool_size=pool_length[i])(drop)\n",
    "        \n",
    "        inputs.append(input_channel)\n",
    "        channel.append(pool)\n",
    "\n",
    "    # feed into 1 more cnn layer\n",
    "    merged = Concatenate(axis=1)([channel[0], channel[1]] )\n",
    "    pool = GlobalMaxPooling1D()(merged)\n",
    "            \n",
    "    # interpretation\n",
    "    outputs = Dense(num_classes, activation='softmax')(pool)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    # compile\n",
    "    model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "    #plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 100, 100)     1000000     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 100, 100)     1000000     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 96, 128)      64128       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 98, 128)      38528       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 96, 128)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 98, 128)      0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 48, 128)      0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 49, 128)      0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 97, 128)      0           max_pooling1d_3[0][0]            \n",
      "                                                                 max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            387         global_max_pooling1d_2[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 2,103,043\n",
      "Trainable params: 2,103,043\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train...\n",
      "Train on 26964 samples, validate on 2997 samples\n",
      "Epoch 1/5\n",
      "26964/26964 [==============================] - 96s 4ms/step - loss: 0.7359 - acc: 0.6762 - val_loss: 0.6813 - val_acc: 0.7157\n",
      "Epoch 2/5\n",
      "26964/26964 [==============================] - 88s 3ms/step - loss: 0.5004 - acc: 0.7981 - val_loss: 0.6478 - val_acc: 0.7287\n",
      "Epoch 3/5\n",
      "26964/26964 [==============================] - 92s 3ms/step - loss: 0.3712 - acc: 0.8557 - val_loss: 0.6406 - val_acc: 0.7294\n",
      "Epoch 4/5\n",
      "26964/26964 [==============================] - 92s 3ms/step - loss: 0.2609 - acc: 0.9037 - val_loss: 0.6596 - val_acc: 0.7217\n",
      "Epoch 5/5\n",
      "26964/26964 [==============================] - 94s 4ms/step - loss: 0.1718 - acc: 0.9413 - val_loss: 0.7124 - val_acc: 0.7087\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "#define model\n",
    "model= define_model(max_len,max_word)\n",
    "# fit model\n",
    "print('Train...')\n",
    "from keras.callbacks import EarlyStopping\n",
    "es=EarlyStopping(monitor='val_loss', min_delta=0, patience=1, verbose=0, mode='auto')\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "mc = ModelCheckpoint('weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', verbose=0, save_best_only=True, mode='min', period=1)\n",
    "history=model.fit([x_train, x_train], y_train, epochs=num_epochs, batch_size=batch_size,validation_split=0.1,callbacks=[es,mc]) #\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3329/3329 [==============================] - 3s 854us/step\n",
      "Test score: 0.700002763195416\n",
      "Test accuracy: 0.7164313607689997\n",
      "F1-macro score for 3 classes: 0.7162152058805763\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.72      0.71      0.72      1089\n",
      "          0       0.75      0.67      0.71      1132\n",
      "          1       0.69      0.77      0.72      1108\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3329\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluate\n",
    "score = model.evaluate([x_test, x_test], y_test_bi,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "y_pred=model.predict([x_test,x_test], batch_size=batch_size)\n",
    "y_pred=np.argmax(y_pred, axis=1)\n",
    "np.place(y_pred,y_pred==2,(-1))\n",
    "\n",
    "# with codecs.open(\"output_multichlCnn.txt\",'a',encoding='utf-8') as file:\n",
    "#     for output in outputs:\n",
    "#         file.writeline(output)\n",
    "\n",
    "from sklearn.metrics import f1_score,classification_report\n",
    "print('F1-macro score for 3 classes:', f1_score(y_test,y_pred,average='macro'))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
