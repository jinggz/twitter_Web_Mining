{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import extracted tweets into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('Samsung_final_brand.csv', sep= ',',encoding='utf-8',error_bad_lines=False,engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45782"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>permalink</th>\n",
       "      <th>Brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>925495649730117632</td>\n",
       "      <td>2017-10-31 23:52</td>\n",
       "      <td># phone gear Anti Gravity Casing for iPhone an...</td>\n",
       "      <td>https://twitter.com/MyPhoneMyWorld/status/9254...</td>\n",
       "      <td>Samsung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>925490587402465280</td>\n",
       "      <td>2017-10-31 23:32</td>\n",
       "      <td>Went to the cell phone repair place and they s...</td>\n",
       "      <td>https://twitter.com/SinamonLance/status/925490...</td>\n",
       "      <td>Samsung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>925483268715184128</td>\n",
       "      <td>2017-10-31 23:03</td>\n",
       "      <td>Like and Share if you want this Haunted Mansio...</td>\n",
       "      <td>https://twitter.com/Siresayshop/status/9254832...</td>\n",
       "      <td>Samsung</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id              date  \\\n",
       "0  925495649730117632  2017-10-31 23:52   \n",
       "1  925490587402465280  2017-10-31 23:32   \n",
       "2  925483268715184128  2017-10-31 23:03   \n",
       "\n",
       "                                                text  \\\n",
       "0  # phone gear Anti Gravity Casing for iPhone an...   \n",
       "1  Went to the cell phone repair place and they s...   \n",
       "2  Like and Share if you want this Haunted Mansio...   \n",
       "\n",
       "                                           permalink    Brand  \n",
       "0  https://twitter.com/MyPhoneMyWorld/status/9254...  Samsung  \n",
       "1  https://twitter.com/SinamonLance/status/925490...  Samsung  \n",
       "2  https://twitter.com/Siresayshop/status/9254832...  Samsung  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(len(data))\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing step\n",
    "# (lower case, word tokenize,stopword removal,punctuation removal, stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "import codecs\n",
    "import csv\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "with codecs.open('samsung_balanced_11098_preprocessed.csv', 'w', 'utf-8') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['id', 'text','polarity','polarity_confidence','subjectivity','subjectivity_confidence','polarityNum'])\n",
    "    for i in range(len(tweets_balanced_csv)):\n",
    "        text = (tweets_balanced_csv['text'].astype(str))[i].lower()\n",
    "\n",
    "        stopwordSet = set(stopwords.words('english'))\n",
    "        textSplit = [word for word in word_tokenize(text) if word not in stopwordSet]\n",
    "        noPunc = [word for word in str(textSplit) if word not in string.punctuation]\n",
    "        words = ''.join(noPunc)\n",
    "        wordStem = [lemmatizer.lemmatize(word) for word in word_tokenize(words)]\n",
    "        alphaOnly = [word for word in wordStem if word.isalpha()]\n",
    "        wordstring = ' '.join(alphaOnly)\n",
    "        writer.writerow([tweets_balanced_csv.id[i], wordstring,tweets_balanced_csv.polarity[i],tweets_balanced_csv.polarity_confidence[i],tweets_balanced_csv.subjectivity[i],tweets_balanced_csv.subjectivity_confidence[i],tweets_balanced_csv.sentiment[i]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed = pd.read_csv('Preprocessed_iphone.csv' ,sep= ',',encoding='utf-8')\n",
    "if data['id'].equals(data_processed['id']) == False:\n",
    "    print (data['id'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
